{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60896d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyao/miniconda3/envs/torch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "import torchtext\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045fc997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e671483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f5d4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 생성\n",
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0246fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import de_core_news_sm\n",
    "import en_core_web_sm\n",
    "\n",
    "# 문장을 토큰화하는 모델을 불러옵니다.\n",
    "spacy_en = en_core_web_sm.load()\n",
    "spacy_de = de_core_news_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51af3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer function 생성\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "    \n",
    "# torchtext의 Field는 데이터를 어떻게 처리할지 조절합니다.\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dedaabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training.tar.gz: 100%|███████████████████████████████████████████████████████████████████████████████████| 1.21M/1.21M [00:05<00:00, 209kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation.tar.gz: 100%|████████████████████████████████████████████████████████████████████████████████| 46.3k/46.3k [00:00<00:00, 75.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmt_task1_test2016.tar.gz: 100%|████████████████████████████████████████████████████████████████████████| 66.2k/66.2k [00:00<00:00, 71.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC,TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9371b267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'büsche',\n",
       " 'vieler',\n",
       " 'nähe',\n",
       " 'der',\n",
       " 'in',\n",
       " 'freien',\n",
       " 'im',\n",
       " 'sind',\n",
       " 'männer',\n",
       " 'weiße',\n",
       " 'junge',\n",
       " 'zwei']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.examples[0].src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b380e8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two',\n",
       " 'young',\n",
       " ',',\n",
       " 'white',\n",
       " 'males',\n",
       " 'are',\n",
       " 'outside',\n",
       " 'near',\n",
       " 'many',\n",
       " 'bushes',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.examples[0].trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2c2d316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data.examples)}')\n",
    "print(f'Number of validation examples: {len(valid_data.examples)}')\n",
    "print(f'Number of testing examples: {len(test_data.examples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d69a6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_freq=2는 2번 이상 등장한 토큰을 출력합니다.\n",
    "# 토큰이 1번만 등장했다면 <unk>로 대체합니다.\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3f546c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator 생성\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aee7aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_iterator:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01702580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  2,   2,   2,  ...,   2,   2,   2],\n",
       "         [  4,   4,   4,  ...,   4,   4,   4],\n",
       "         [592, 290, 154,  ...,   0, 312, 403],\n",
       "         ...,\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1]]),\n",
       " torch.Size([28, 128]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.src, x.src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16fcd38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  2,   2,   2,  ...,   2,   2,   2],\n",
       "         [  7,   4,   4,  ..., 196,   4,   4],\n",
       "         [  9,   9,  64,  ...,  17,  61,   9],\n",
       "         ...,\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1],\n",
       "         [  1,   1,   1,  ...,   1,   1,   1]]),\n",
       " torch.Size([28, 128]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.trg, x.src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfee6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb71300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # embedding: 입력값을 emd_dim 벡터로 변경\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        # embedding을 입력받아 hid_dim 크기의 hidden state, cell 출력\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # sre: [src_len, batch_size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        # initial hidden state는 zero tensor\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        # output: [src_len, batch_size, hid dim * n directions]\n",
    "        # hidden: [n layers * n directions, batch_size, hid dim]\n",
    "        # cell: [n layers * n directions, batch_size, hid dim]\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9775dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # content vector를 입력받아 emb_dim 출력\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        # embedding을 입력받아 hid_dim 크기의 hidden state, cell 출력\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input: [batch_size]\n",
    "        # hidden: [n layers * n directions, batch_size, hid dim]\n",
    "        # cell: [n layers * n directions, batch_size, hid dim]\n",
    "\n",
    "        input = input.unsqueeze(0) # input: [1, batch_size], 첫번째 input은 <SOS>\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input)) # [1, batch_size, emd dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output: [seq len, batch_size, hid dim * n directions]\n",
    "        # hidden: [n layers * n directions, batch size, hid dim]\n",
    "        # cell: [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        prediction = self.fc_out(output.squeeze(0)) # [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fabb9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        # encoder와 decoder의 hid_dim이 일치하지 않는 경우 에러메세지\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            'Hidden dimensions of encoder decoder must be equal'\n",
    "        # encoder와 decoder의 hid_dim이 일치하지 않는 경우 에러메세지\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            'Encoder and decoder must have equal number of layers'\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [src len, batch size]\n",
    "        # trg: [trg len, batch size]\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0] # 타겟 토큰 길이 얻기\n",
    "        trg_vocab_size = self.decoder.output_dim # context vector의 차원\n",
    "\n",
    "        # decoder의 output을 저장하기 위한 tensor\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # initial hidden state\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # 첫 번째 입력값 <sos> 토큰\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1,trg_len): # <eos> 제외하고 trg_len-1 만큼 반복\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # prediction 저장\n",
    "            outputs[t] = output\n",
    "\n",
    "            # teacher forcing을 사용할지, 말지 결정\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # 가장 높은 확률을 갖은 값 얻기\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            # teacher forcing의 경우에 다음 lstm에 target token 입력\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb747f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647f8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b7aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb6f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 지정\n",
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "enc_emb_dim = 256 # 임베딩 차원\n",
    "dec_emb_dim = 256\n",
    "hid_dim = 512 # hidden state 차원\n",
    "n_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04ce4c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "enc = Encoder(input_dim, enc_emb_dim, hid_dim, n_layers, enc_dropout)\n",
    "dec = Decoder(output_dim, dec_emb_dim, hid_dim, n_layers, dec_dropout)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bdf3335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 초기화\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3668a64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,898,501 trainableparameters\n"
     ]
    }
   ],
   "source": [
    "# 모델의 학습가능한 파라미터 수 측정\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainableparameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd2638c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# loss function\n",
    "# pad에 해당하는 index는 무시합니다.\n",
    "trg_pad_idx = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87c938b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 함수\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src,trg) # [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim) # loss 계산을 위해 1d로 변경\n",
    "        trg = trg[1:].view(-1) # loss 계산을 위해 1d로 변경\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # 기울기 clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cab264fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            # output: [trg len, batch size, output dim]\n",
    "            output = model(src, trg, 0) # teacher forcing off\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim) # [(trg len -1) * batch size, output dim]\n",
    "            trg = trg[1:].view(-1) # [(trg len -1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19568132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count training time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "422a48de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 20m 17s\n",
      "\tTrain Loss: 5.065 | Train PPL: 158.309\n",
      "\t Val. Loss: 4.912 |  Val. PPL: 135.845\n",
      "Epoch: 02 | Time: 19m 30s\n",
      "\tTrain Loss: 4.494 | Train PPL:  89.458\n",
      "\t Val. Loss: 4.790 |  Val. PPL: 120.273\n",
      "Epoch: 03 | Time: 18m 31s\n",
      "\tTrain Loss: 4.231 | Train PPL:  68.781\n",
      "\t Val. Loss: 4.652 |  Val. PPL: 104.775\n",
      "Epoch: 04 | Time: 19m 6s\n",
      "\tTrain Loss: 4.009 | Train PPL:  55.108\n",
      "\t Val. Loss: 4.518 |  Val. PPL:  91.622\n",
      "Epoch: 05 | Time: 18m 46s\n",
      "\tTrain Loss: 3.854 | Train PPL:  47.158\n",
      "\t Val. Loss: 4.404 |  Val. PPL:  81.806\n",
      "Epoch: 06 | Time: 19m 8s\n",
      "\tTrain Loss: 3.732 | Train PPL:  41.752\n",
      "\t Val. Loss: 4.337 |  Val. PPL:  76.459\n",
      "Epoch: 07 | Time: 19m 4s\n",
      "\tTrain Loss: 3.621 | Train PPL:  37.360\n",
      "\t Val. Loss: 4.169 |  Val. PPL:  64.655\n",
      "Epoch: 08 | Time: 19m 15s\n",
      "\tTrain Loss: 3.497 | Train PPL:  33.012\n",
      "\t Val. Loss: 4.138 |  Val. PPL:  62.674\n",
      "Epoch: 09 | Time: 18m 59s\n",
      "\tTrain Loss: 3.377 | Train PPL:  29.297\n",
      "\t Val. Loss: 4.039 |  Val. PPL:  56.793\n",
      "Epoch: 10 | Time: 19m 32s\n",
      "\tTrain Loss: 3.294 | Train PPL:  26.961\n",
      "\t Val. Loss: 3.994 |  Val. PPL:  54.285\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "num_epochs = 10\n",
    "clip = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "   \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, clip)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d55fc4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.030 | Test PPL:  56.276 |\n"
     ]
    }
   ],
   "source": [
    "# best val loss일 때의 가중치를 불러옵니다.\n",
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "# test loss를 측정합니다.\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "369a77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# epoch_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_iterator):\n",
    "\n",
    "\n",
    "#         loss = criterion(output, trg)\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f1a8ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = batch.src\n",
    "trg = batch.trg\n",
    "\n",
    "# output: [trg len, batch size, output dim]\n",
    "output = model(src, trg, 0) # teacher forcing off\n",
    "# output_dim = output.shape[-1]\n",
    "# output = output[1:].view(-1, output_dim) # [(trg len -1) * batch size, output dim]\n",
    "# trg = trg[1:].view(-1) # [(trg len -1) * batch size, output dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0ee9fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'anstarrt',\n",
       " 'etwas',\n",
       " 'der',\n",
       " ',',\n",
       " 'hut',\n",
       " 'orangefarbenen',\n",
       " 'einem',\n",
       " 'mit',\n",
       " 'mann',\n",
       " 'ein']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.dataset.examples[0].src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59bea81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.dataset.examples[0].trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b5bd210f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
       "         [  16,  110,    4,  ...,    4,   24,   16],\n",
       "         [1909,   19,   34,  ...,   14,   14,   30],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]),\n",
       " torch.Size([14, 128]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.trg,  batch.trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84fb6478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
       "         [   4,    4,    4,  ...,    4,  714,    4],\n",
       "         [ 123,   91, 3449,  ...,  669,   12, 1642],\n",
       "         ...,\n",
       "         [6787,   41,   26,  ...,    1,    1,    1],\n",
       "         [  18,  105,    5,  ...,    1,    1,    1],\n",
       "         [   3,    3,    3,  ...,    1,    1,    1]]),\n",
       " torch.Size([10, 128]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src, src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b8c795b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
       "         [  16,  110,    4,  ...,    4,   24,   16],\n",
       "         [1909,   19,   34,  ...,   14,   14,   30],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]),\n",
       " torch.Size([14, 128]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg, trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "86c183ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          ...,\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
       "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
       " \n",
       "         [[  3.1175,  -7.6537,  -8.2521,  ...,  -3.8952,  -6.4166,  -4.5838],\n",
       "          [  3.2495,  -7.8209,  -8.5892,  ...,  -5.6360,  -7.3985,  -4.8339],\n",
       "          [  4.1826,  -7.6452,  -9.0038,  ...,  -9.6126,  -7.0450,  -4.9051],\n",
       "          ...,\n",
       "          [  4.2122,  -7.3043,  -8.6344,  ...,  -8.0330,  -6.8491,  -5.6908],\n",
       "          [  4.3218,  -7.6512,  -8.8377,  ..., -11.0679,  -7.3509,  -5.3674],\n",
       "          [  3.0761,  -7.3838,  -8.5991,  ...,  -6.5524,  -7.1208,  -5.0551]],\n",
       " \n",
       "         [[  5.1792,  -8.4672,  -9.5905,  ...,  -8.4732,  -7.1803,  -7.7852],\n",
       "          [  4.7680,  -8.8799,  -9.8944,  ...,  -8.9553,  -8.1275,  -7.8520],\n",
       "          [  4.5185,  -8.9688,  -9.8259,  ..., -12.2365,  -6.0684,  -4.9997],\n",
       "          ...,\n",
       "          [  4.3856,  -7.3184,  -8.6149,  ...,  -9.0487,  -5.2651,  -6.8919],\n",
       "          [  5.4616,  -8.2450,  -9.7886,  ..., -12.9242,  -6.4300,  -6.4945],\n",
       "          [  5.1164,  -7.1366,  -8.5604,  ..., -11.7262,  -7.3961,  -6.7967]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  3.2016,  -6.9260,  -8.4251,  ...,  -6.3676,  -6.2349,  -6.4279],\n",
       "          [  3.0459,  -7.3676,  -8.6872,  ...,  -6.2622,  -7.2317,  -6.3447],\n",
       "          [  3.7920,  -7.6122,  -8.9966,  ...,  -5.6894,  -5.3361,  -6.0367],\n",
       "          ...,\n",
       "          [  3.2000,  -6.7376,  -7.8394,  ...,  -5.8632,  -7.0545,  -6.4617],\n",
       "          [  3.2087,  -6.9693,  -7.8981,  ...,  -6.9538,  -5.6723,  -5.7270],\n",
       "          [  3.6584,  -6.6987,  -7.4536,  ...,  -7.7907,  -6.2425,  -7.0898]],\n",
       " \n",
       "         [[  3.2264,  -6.9995,  -8.4018,  ...,  -6.5144,  -6.1496,  -6.4028],\n",
       "          [  3.0686,  -7.1493,  -8.5627,  ...,  -5.7095,  -6.9031,  -5.9796],\n",
       "          [  3.4763,  -7.2369,  -8.6639,  ...,  -5.9997,  -4.9301,  -4.8356],\n",
       "          ...,\n",
       "          [  3.3511,  -6.7506,  -7.7799,  ...,  -6.3802,  -6.9806,  -6.4600],\n",
       "          [  3.3327,  -6.7409,  -7.6756,  ...,  -6.7221,  -5.6563,  -5.7165],\n",
       "          [  3.6909,  -6.7719,  -7.6285,  ...,  -7.8613,  -6.2101,  -6.9558]],\n",
       " \n",
       "         [[  3.2343,  -6.7990,  -8.2927,  ...,  -5.9676,  -6.0943,  -6.0371],\n",
       "          [  3.1357,  -6.9917,  -8.4034,  ...,  -5.9676,  -6.7889,  -5.8129],\n",
       "          [  3.3721,  -7.1497,  -8.2753,  ...,  -6.9066,  -5.1844,  -4.8611],\n",
       "          ...,\n",
       "          [  3.4674,  -6.7637,  -7.7801,  ...,  -6.4948,  -6.8680,  -6.4336],\n",
       "          [  3.4525,  -6.5840,  -7.5228,  ...,  -6.8600,  -5.7883,  -5.6651],\n",
       "          [  3.7551,  -6.8546,  -7.7888,  ...,  -7.8728,  -6.2247,  -6.9034]]],\n",
       "        grad_fn=<CopySlices>),\n",
       " torch.Size([14, 128, 5893]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ceead064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1664, 5893])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim = output.shape[-1]\n",
    "output[1:].view(-1, output_dim).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "13d1a548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1792, 5893])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim = output.shape[-1]\n",
    "output.view(-1, output_dim).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5431d443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'two',\n",
       " 'medium',\n",
       " 'sized',\n",
       " 'dogs',\n",
       " 'run',\n",
       " 'across',\n",
       " 'the',\n",
       " 'snow',\n",
       " '.',\n",
       " '<eos>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[list(TRG.vocab.stoi)[i] for i in trg[:,0].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a0352092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " '.',\n",
       " 'schnee',\n",
       " 'den',\n",
       " 'über',\n",
       " 'laufen',\n",
       " 'hunde',\n",
       " 'mittelgroße',\n",
       " 'zwei',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[list(SRC.vocab.stoi)[i] for i in src[:,0].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da287d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
